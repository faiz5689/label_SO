# ./modules/components/evaluation_form.py

import streamlit as st
from modules.components.display import get_model_response, display_question_details, render_html


def display_model_evaluation_form(model_name, question_key, current_question, with_image=False):
    """Display the evaluation form for a model with hierarchical layout

    Args:
        model_name (str): Name of the model (GPT, Gemini, Llama)
        question_key (str): Unique key for this question
        current_question (dict): The question data
        with_image (bool, optional): Whether to evaluate with-image response. Defaults to False.

    Returns:
        dict: The evaluation data
    """
    # Get current state for this model (with separate keys for with/without image)
    model_key = f"{model_name}_{'with' if with_image else 'without'}_image_evaluation"
    model_data = st.session_state.current_evaluation.get(model_key, {})

    # Get model response
    model_response = get_model_response(current_question, model_name, with_image)
    subtitle = f"{model_name} Response ({('With' if with_image else 'Without')} Image)"

    # Display the model's response
    st.subheader(subtitle)
    render_html(model_response)

    # Evaluation form fields
    st.subheader(f"Evaluate {model_name}'s Response ({('With' if with_image else 'Without')} Image)")

    # Create hierarchical form with visual indentation using markdown
    with st.container():
        # Question 1: Correctness
        st.markdown(f"### 1. Is the answer generated by {model_name} correct?")
        is_correct = st.radio(
            f"Is the {model_name} answer correct?",
            ["Yes", "No"],
            index=0 if model_data.get("correctness", {}).get("is_correct", True) else 1,
            key=f"{question_key}_correct_{model_name}_{with_image}"
        )

        # Show issues only if answer is not correct
        correctness_issues = []
        code_issues_types = []
        non_functional_types = []

        if is_correct == "No":
            # First level indentation
            st.markdown("&nbsp;&nbsp;&nbsp;&nbsp;• If not, what are its issues?")
            # Use columns for the first level only
            _, col2 = st.columns([0.1, 0.9])
            with col2:
                correctness_issues = st.multiselect(
                    "Select all issues that apply:",
                    ["Incorrect (Factual)", "Incorrect (Code)", "Incorrect (Concept)", "Incorrect (Terminology)"],
                    default=model_data.get("correctness", {}).get("issues", []),
                    key=f"{question_key}_correct_issues_{model_name}_{with_image}"
                )

                # If code issues are selected, ask for specific code issues
                if "Incorrect (Code)" in correctness_issues:
                    # Second level indentation using markdown
                    st.markdown("&nbsp;&nbsp;&nbsp;&nbsp;• If any code issues, what are the issues?")
                    code_issues_types = st.multiselect(
                        "Select all code issues that apply:",
                        ["Syntax Error", "Non-Functional", "Runtime Error"],
                        default=model_data.get("code_issues", {}).get("types", []),
                        key=f"{question_key}_code_issues_types_{model_name}_{with_image}"
                    )

                    # If Non-Functional is selected, ask for specific non-functional issues
                    if "Non-Functional" in code_issues_types:
                        # Third level indentation using markdown
                        st.markdown("&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• Select specific non-functional issues:")
                        non_functional_types = st.multiselect(
                            "Select all that apply:",
                            ["Wrong API/Library/Function", "Wrong Logic", "Incomplete"],
                            default=model_data.get("code_issues", {}).get("non_functional_types", []),
                            key=f"{question_key}_non_functional_types_{model_name}_{with_image}"
                        )

    with st.container():
        # Question 2: Consistency
        st.markdown(f"### 2. Is the answer generated by {model_name} Consistent?")
        is_consistent = st.radio(
            f"Is the {model_name} answer consistent?",
            ["Yes", "No"],
            index=0 if model_data.get("consistency", {}).get("is_consistent", True) else 1,
            key=f"{question_key}_consistent_{model_name}_{with_image}"
        )

        # Show issues only if answer is not consistent
        consistency_issues = []
        if is_consistent == "No":
            # First level indentation
            st.markdown("&nbsp;&nbsp;&nbsp;&nbsp;• If not, what are its issues?")
            # Use columns for the first level only
            _, col2 = st.columns([0.1, 0.9])
            with col2:
                consistency_issues = st.multiselect(
                    "Select all issues that apply:",
                    ["Inconsistency (Factual)", "Inconsistency (Code)", "Inconsistency (Concept)",
                    "Inconsistency (Terminology)", "Inconsistency (Number of solutions)"],
                    default=model_data.get("consistency", {}).get("issues", []),
                    key=f"{question_key}_consistency_issues_{model_name}_{with_image}"
                )

    with st.container():
        # Question 3: Comprehensiveness
        st.markdown(f"### 3. Is the answer generated by {model_name} comprehensive?")
        is_comprehensive = st.radio(
            "Select one:",
            ["Yes", "No"],
            index=0 if model_data.get("is_comprehensive", True) else 1,
            key=f"{question_key}_comprehensive_{model_name}_{with_image}"
        )

    with st.container():
        # Question 4: Conciseness
        st.markdown(f"### 4. Is the answer concise?")
        is_concise = st.radio(
            "Select one:",
            ["Yes", "No"],
            index=0 if model_data.get("conciseness", {}).get("is_concise", True) else 1,
            key=f"{question_key}_concise_{model_name}_{with_image}"
        )

        # Show issues only if answer is not concise
        conciseness_issues = []
        if is_concise == "No":
            # First level indentation
            st.markdown("&nbsp;&nbsp;&nbsp;&nbsp;• If not, what are the issues?")
            # Use columns for the first level only
            _, col2 = st.columns([0.1, 0.9])
            with col2:
                conciseness_issues = st.multiselect(
                    "Select all issues that apply:",
                    ["Not Concise (Redundant)", "Not Concise (Excess)", "Not Concise (Irrelevant)"],
                    default=model_data.get("conciseness", {}).get("issues", []),
                    key=f"{question_key}_conciseness_issues_{model_name}_{with_image}"
                )

    with st.container():
        # Question 5: Usefulness
        st.markdown("### 5. Usefulness Rating:")
        st.markdown("&nbsp;&nbsp;&nbsp;&nbsp;• On a scale of 1 to 5, how useful is the answer?")
        usefulness_rating = st.radio(
            "Select a rating:",
            [1, 2, 3, 4, 5],
            index=model_data.get("usefulness_rating", 3) - 1,
            horizontal=True,
            key=f"{question_key}_usefulness_{model_name}_{with_image}"
        )

    # Additional notes
    with st.container():
        st.markdown("### Additional Notes")
        notes = st.text_area(
            f"Any additional comments or observations about {model_name}'s response:",
            value=model_data.get("notes", ""),
            height=100,
            key=f"{question_key}_notes_{model_name}_{with_image}"
        )

    # Create and return evaluation data
    evaluation_data = {
        "correctness": {
            "is_correct": is_correct == "Yes",
            "issues": correctness_issues
        },
        "consistency": {
            "is_consistent": is_consistent == "Yes",
            "issues": consistency_issues
        },
        "is_comprehensive": is_comprehensive == "Yes",
        "conciseness": {
            "is_concise": is_concise == "Yes",
            "issues": conciseness_issues
        },
        "usefulness_rating": usefulness_rating,
        "code_issues": {
            "has_issues": "Incorrect (Code)" in correctness_issues,
            "types": code_issues_types,
            "non_functional_types": non_functional_types
        },
        "notes": notes
    }

    # Save to session state using the with/without image specific key
    st.session_state.current_evaluation[model_key] = evaluation_data

    # Update the post evaluations
    post_id = st.session_state.current_evaluation["post_id"]
    st.session_state.post_evaluations[post_id] = st.session_state.current_evaluation

    return evaluation_data


def model_evaluation_tabs(model_name, current_question, question_key):
    """Create tabs for a specific model's evaluation

    Args:
        model_name (str): Name of the model (GPT, Gemini, Llama)
        current_question (dict): The question data
        question_key (str): Unique key for this question
    """
    # Add our navigation tabs at the top for better tab organization
    st.subheader(f"{model_name} Model Evaluation")

    # Create tabs for with/without image evaluation
    tabs = st.tabs([f"Without Image", f"With Image"])

    # Get accepted answer
    accepted_answer_body = current_question.get("accepted_answer", {}).get("body", "")

    # Tab 1: Without Image
    with tabs[0]:
        # Display question details without image
        display_question_details(current_question, current_question.get("post_id"), show_image=False)

        # Display accepted answer
        st.subheader("Accepted Answer")
        render_html(accepted_answer_body)

        # Display evaluation form for without image
        display_model_evaluation_form(model_name, question_key, current_question, with_image=False)

    # Tab 2: With Image
    with tabs[1]:
        # Display question details with image
        display_question_details(current_question, current_question.get("post_id"), show_image=True)

        # Display accepted answer
        st.subheader("Accepted Answer")
        render_html(accepted_answer_body)

        # Display evaluation form for with image
        display_model_evaluation_form(model_name, question_key, current_question, with_image=True)
